# =============================================================================
# INFRASTRUCTURE INTEGRATION FOR STAGE 1 CLEANING PIPELINE
# =============================================================================
# This compose file integrates with centralized infrastructure services:
# - PostgreSQL (for job_registry database)
# - Redis (for Celery broker + checkpoints)
# - Traefik (for API routing)
# - Prometheus/Grafana (for observability)
#
# Stage 1 Resource Allocation:
# - Celery DB: 0 (calculated as (1-1)*2 = 0)
# - Cache DB: 1 (calculated as (1-1)*2+1 = 1)
# - PostgreSQL DB: stage1_cleaning
# - Traefik Route: /api/v1/cleaning/*
# =============================================================================

networks:
  storytelling:
    external: true
    name: storytelling

services:
  # ===========================================================================
  # INFRASTRUCTURE AVAILABILITY CHECK
  # ===========================================================================
  # Ensures infrastructure services are ready before starting Stage 1 services
  infrastructure-check:
    image: busybox:latest
    container_name: cleaning-infrastructure-check
    networks:
      - storytelling
    command: >
      sh -c "
        echo '=== Checking Stage 1 Infrastructure Availability ===' &&
        echo 'Checking Redis Broker (redis-broker:6379)...' &&
        nc -zv redis-broker 6379 &&
        echo 'Checking Redis Cache (redis-cache:6379)...' &&
        nc -zv redis-cache 6379 &&
        echo 'Checking PostgreSQL (postgres:5432)...' &&
        nc -zv postgres 5432 &&
        echo '=== All infrastructure services available ===' &&
        exit 0
      "
    restart: "no"

  # ===========================================================================
  # ORCHESTRATOR SERVICE (FastAPI)
  # ===========================================================================
  # Main API service for batch submission, job management, health checks
  orchestrator-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cleaning-orchestrator
    hostname: cleaning-orchestrator
    networks:
      - storytelling
    volumes:
      - ./src:/app/src:ro  # Hot-reload source code
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./data:/app/data:rw
      - ./.cache:/app/.cache
      # Shared data directory for inter-stage communication
      - /shared/stage1:/shared/stage1:rw
    environment:
      # General
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MODE=${MODE:-production}

      # PostgreSQL (Job Registry)
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_DB=${POSTGRES_DB:-stage1_cleaning}
      - POSTGRES_USER=${POSTGRES_USER:-stage1_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_POOL_MIN_SIZE=${POSTGRES_POOL_MIN_SIZE:-2}
      - POSTGRES_POOL_MAX_SIZE=${POSTGRES_POOL_MAX_SIZE:-10}

      # Redis (Celery Broker + Checkpoints)
      - REDIS_HOST=${REDIS_HOST:-redis-broker}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://redis-broker:6379/0}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://redis-cache:6379/1}
      - REDIS_CACHE_URL=${REDIS_CACHE_URL:-redis://redis-cache:6379/1}
      - REDIS_CACHE_DB=${REDIS_CACHE_DB:-1}

      # Batch Lifecycle Management
      - BATCH_LIFECYCLE_ENABLED=${BATCH_LIFECYCLE_ENABLED:-true}
      - CHECKPOINT_ENABLED=${CHECKPOINT_ENABLED:-true}
      - CHECKPOINT_INTERVAL=${CHECKPOINT_INTERVAL:-10}
      - CHECKPOINT_TTL_SECONDS=${CHECKPOINT_TTL_SECONDS:-86400}
      - DEFAULT_BATCH_SIZE=${DEFAULT_BATCH_SIZE:-100}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-10000}

      # Resource Management
      - IDLE_TIMEOUT_SECONDS=${IDLE_TIMEOUT_SECONDS:-300}
      - CLEANUP_ON_IDLE=${CLEANUP_ON_IDLE:-true}
      - CPU_THRESHOLD_PERCENT=${CPU_THRESHOLD_PERCENT:-95}
      - MEMORY_THRESHOLD_PERCENT=${MEMORY_THRESHOLD_PERCENT:-90}

      # CloudEvents Multi-Backend
      - CLOUDEVENTS_ENABLED=${CLOUDEVENTS_ENABLED:-true}
      - REDIS_STREAMS_ENABLED=${REDIS_STREAMS_ENABLED:-true}
      - REDIS_STREAMS_STREAM_NAME=${REDIS_STREAMS_STREAM_NAME:-stage1:cleaning:events}
      - WEBHOOK_ENABLED=${WEBHOOK_ENABLED:-false}
      - WEBHOOK_URLS=${WEBHOOK_URLS:-}

      # Metadata Registry Integration
      - METADATA_REGISTRY_ENABLED=${METADATA_REGISTRY_ENABLED:-true}
      - METADATA_PRIMARY_BACKEND=${METADATA_PRIMARY_BACKEND:-postgresql}
      - METADATA_ENABLE_REDIS_CACHE=${METADATA_ENABLE_REDIS_CACHE:-true}
      - METADATA_SERVICE_PASSWORD=${METADATA_SERVICE_PASSWORD}

      # Observability
      - OTEL_ENABLED=${OTEL_ENABLED:-false}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://tempo:4317}
      - OTEL_SERVICE_NAME=${OTEL_SERVICE_NAME:-stage1-cleaning-orchestrator}
      - JSON_LOGS=${JSON_LOGS:-true}

    labels:
      # Stage identification (REQUIRED for service discovery)
      - "stage=1"
      - "service=orchestrator"
      - "com.docker.compose.project=stage1-cleaning-pipeline"

      # Traefik routing (handled by centralized Traefik config)
      - "traefik.enable=true"
      - "traefik.http.routers.cleaning.rule=PathPrefix(`/api/v1/cleaning`)"
      - "traefik.http.services.cleaning.loadbalancer.server.port=8000"
      - "traefik.http.routers.cleaning.middlewares=strip-cleaning"
      - "traefik.http.middlewares.strip-cleaning.stripprefix.prefixes=/api/v1/cleaning"

      # Prometheus metrics
      - "prometheus.scrape=true"
      - "prometheus.port=8000"
      - "prometheus.path=/metrics"

    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
          devices:
            # GPU support (if available)
            - capabilities: [gpu]
              count: all

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

    restart: unless-stopped

    depends_on:
      infrastructure-check:
        condition: service_completed_successfully

  # ===========================================================================
  # CELERY WORKER SERVICE
  # ===========================================================================
  # Background task processing for batch document cleaning
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cleaning-celery-worker
    hostname: cleaning-celery-worker
    networks:
      - storytelling
    command: celery -A src.celery_app worker --loglevel=${LOG_LEVEL:-INFO} --pool=prefork --concurrency=${CELERY_WORKER_CONCURRENCY:-4}
    volumes:
      - ./src:/app/src:ro  # Hot-reload source code
      - ./data:/app/data:rw
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./.cache:/app/.cache
      # Shared data directory
      - /shared/stage1:/shared/stage1:rw
    environment:
      # General
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MODE=${MODE:-production}

      # PostgreSQL (Job Registry)
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_DB=${POSTGRES_DB:-stage1_cleaning}
      - POSTGRES_USER=${POSTGRES_USER:-stage1_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_POOL_MIN_SIZE=${POSTGRES_POOL_MIN_SIZE:-2}
      - POSTGRES_POOL_MAX_SIZE=${POSTGRES_POOL_MAX_SIZE:-10}

      # Redis (Celery Broker + Checkpoints)
      - REDIS_HOST=${REDIS_HOST:-redis-broker}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://redis-broker:6379/0}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://redis-cache:6379/1}
      - REDIS_CACHE_URL=${REDIS_CACHE_URL:-redis://redis-cache:6379/1}
      - REDIS_CACHE_DB=${REDIS_CACHE_DB:-1}

      # Celery Worker Config
      - CELERY_WORKER_CONCURRENCY=${CELERY_WORKER_CONCURRENCY:-4}
      - CELERY_TASK_ACKS_LATE=${CELERY_TASK_ACKS_LATE:-true}
      - CELERY_WORKER_PREFETCH_MULTIPLIER=${CELERY_WORKER_PREFETCH_MULTIPLIER:-1}
      - CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP=${CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP:-true}

      # Batch Lifecycle Management
      - BATCH_LIFECYCLE_ENABLED=${BATCH_LIFECYCLE_ENABLED:-true}
      - CHECKPOINT_ENABLED=${CHECKPOINT_ENABLED:-true}
      - CHECKPOINT_INTERVAL=${CHECKPOINT_INTERVAL:-10}
      - CHECKPOINT_TTL_SECONDS=${CHECKPOINT_TTL_SECONDS:-86400}

      # Resource Management
      - IDLE_TIMEOUT_SECONDS=${IDLE_TIMEOUT_SECONDS:-300}
      - CLEANUP_ON_IDLE=${CLEANUP_ON_IDLE:-true}
      - CPU_THRESHOLD_PERCENT=${CPU_THRESHOLD_PERCENT:-95}
      - MEMORY_THRESHOLD_PERCENT=${MEMORY_THRESHOLD_PERCENT:-90}
      - ENABLE_LOW_RESOURCE_MODE=${ENABLE_LOW_RESOURCE_MODE:-false}
      - GPU_MEMORY_FRACTION=${GPU_MEMORY_FRACTION:-0.8}

      # CloudEvents Multi-Backend
      - CLOUDEVENTS_ENABLED=${CLOUDEVENTS_ENABLED:-true}
      - REDIS_STREAMS_ENABLED=${REDIS_STREAMS_ENABLED:-true}
      - REDIS_STREAMS_STREAM_NAME=${REDIS_STREAMS_STREAM_NAME:-stage1:cleaning:events}
      - REDIS_STREAMS_MAX_LEN=${REDIS_STREAMS_MAX_LEN:-10000}
      - REDIS_STREAMS_TTL_SECONDS=${REDIS_STREAMS_TTL_SECONDS:-86400}
      - WEBHOOK_ENABLED=${WEBHOOK_ENABLED:-false}
      - WEBHOOK_URLS=${WEBHOOK_URLS:-}
      - WEBHOOK_TIMEOUT_SECONDS=${WEBHOOK_TIMEOUT_SECONDS:-30}
      - WEBHOOK_RETRY_ATTEMPTS=${WEBHOOK_RETRY_ATTEMPTS:-3}
      - KAFKA_ENABLED=${KAFKA_ENABLED:-false}
      - NATS_ENABLED=${NATS_ENABLED:-false}
      - RABBITMQ_ENABLED=${RABBITMQ_ENABLED:-false}

      # Metadata Registry Integration
      - METADATA_REGISTRY_ENABLED=${METADATA_REGISTRY_ENABLED:-true}
      - METADATA_PRIMARY_BACKEND=${METADATA_PRIMARY_BACKEND:-postgresql}
      - METADATA_ENABLE_REDIS_CACHE=${METADATA_ENABLE_REDIS_CACHE:-true}
      - METADATA_MAX_RETRIES=${METADATA_MAX_RETRIES:-3}
      - METADATA_RETRY_DELAY_SECONDS=${METADATA_RETRY_DELAY_SECONDS:-2}
      - METADATA_SERVICE_PASSWORD=${METADATA_SERVICE_PASSWORD}

      # Observability
      - OTEL_ENABLED=${OTEL_ENABLED:-false}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://tempo:4317}
      - OTEL_SERVICE_NAME=${OTEL_SERVICE_NAME:-stage1-cleaning-celery-worker}
      - JSON_LOGS=${JSON_LOGS:-true}

    labels:
      # Stage identification
      - "stage=1"
      - "service=celery-worker"
      - "com.docker.compose.project=stage1-cleaning-pipeline"

      # Prometheus metrics (if worker exposes metrics)
      - "prometheus.scrape=false"

    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
          devices:
            # GPU support for NER processing
            - capabilities: [gpu]
              count: all

    healthcheck:
      test: ["CMD-SHELL", "celery -A src.celery_app inspect ping || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s

    restart: unless-stopped

    depends_on:
      infrastructure-check:
        condition: service_completed_successfully
      orchestrator-service:
        condition: service_healthy

# =============================================================================
# SHARED VOLUMES (if needed)
# =============================================================================
# Note: Most data is persisted in infrastructure services or shared volumes
volumes:
  # Local cache for spaCy models
  spacy_cache:
