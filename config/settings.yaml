# config/settings.yaml

general:
  log_level: INFO # Set to INFO for production readiness, DEBUG for development
  gpu_enabled: True # Set to True to leverage your RTX A4000

ingestion_service:
  port: 8000
  model_name: "en_core_web_trf" # The spaCy model to use for NER. Change if needed.
  model_cache_dir: "/app/.cache/spacy" # Path for spaCy to cache models.
  dateparser_languages: ["en"] # Languages for dateparser to consider.
  batch_processing_threads: 4 # Number of threads for CLI batch processing
  
  # Language detection settings
  langdetect_confidence_threshold: 0.9 # Minimum confidence for language detection
  
  # Text Cleaning Pipeline Configuration
  # Each cleaning step can be individually enabled/disabled
  cleaning_pipeline:
    # HTML and markup cleaning
    remove_html_tags: true
    
    # Whitespace normalization
    normalize_whitespace: true
    
    # Encoding fixes
    fix_encoding: true # Uses ftfy to fix mojibake and encoding issues
    
    # Punctuation normalization
    normalize_punctuation: true
    normalize_unicode_dashes: true
    normalize_smart_quotes: true
    remove_excessive_punctuation: true # Remove repeated !!!!, ????, etc.
    add_space_after_punctuation: true
    
    # Unit and currency standardization
    standardize_units: true
    standardize_currency: true
    
    # Typo correction settings
    enable_typo_correction: true
    typo_correction:
      min_word_length: 2 # Don't check words shorter than this
      max_word_length: 15 # Don't check words longer than this (likely proper nouns)
      skip_capitalized_words: true # Skip words that start with capital letter (proper nouns)
      skip_mixed_case: true # Skip words like "iPhone", "McDonald's"
      use_ner_entities: true # Use spaCy NER to skip entity words (CRITICAL for "San Francisco")
      confidence_threshold: 0.7 # How confident spell checker must be (0.0-1.0)
      
  # Entity Recognition Settings
  entity_recognition:
    enabled: true
    entity_types_to_extract: ["PERSON", "ORG", "GPE", "LOC", "DATE", "TIME", "MONEY", "PERCENT"]
    # GPE = Geopolitical Entity (countries, cities, states)
    # LOC = Non-GPE locations (mountains, water bodies)

celery:
  broker_url: "redis://redis:6379/0"
  result_backend: "redis://redis:6379/0"
  task_acks_late: true
  worker_prefetch_multiplier: 1
  worker_concurrency: 4
  task_annotations:
    '*':
      rate_limit: '300/m'

storage:
  backend: "jsonl"
  enabled_backends: ["jsonl"]
  
  jsonl:
    output_path: "/app/data/processed_articles.jsonl"

  elasticsearch:
    host: "elasticsearch"
    port: 9200
    scheme: "http"
    index_name: "news_articles"
    api_key: null

  postgresql:
    host: "postgres"
    port: 5432
    dbname: "newsdb"
    user: "user"
    password: "password"
    table_name: "processed_articles"

logging:
  version: 1
  disable_existing_loggers: false
  formatters:
    json:
      class: pythonjsonlogger.jsonlogger.JsonFormatter
      format: "%(levelname)s %(asctime)s %(filename)s %(funcName)s %(lineno)d %(message)s"
  handlers:
    console:
      class: logging.StreamHandler
      formatter: json
      stream: ext://sys.stdout
    ingestion_file:
      class: logging.handlers.RotatingFileHandler
      formatter: json
      filename: /app/logs/ingestion_service.jsonl
      maxBytes: 10485760
      backupCount: 5
  root:
    handlers: [console]
    level: INFO
  loggers:
    ingestion_service:
      handlers: [ingestion_file, console]
      level: INFO
      propagate: false

# =============================================================================
# BATCH LIFECYCLE MANAGEMENT (NEW)
# =============================================================================

batch_processing:
  # Checkpoint configuration
  checkpoint_enabled: true
  checkpoint_interval: 10  # Save checkpoint every N documents
  checkpoint_ttl_seconds: 86400  # 24 hours

  # Default batch size for processing
  default_batch_size: 100
  max_batch_size: 10000

  # Progressive persistence
  save_intermediate_results: true

# =============================================================================
# RESOURCE MANAGEMENT (NEW)
# =============================================================================

resource_management:
  # Idle detection and cleanup
  idle_timeout_seconds: 300  # 5 minutes
  cleanup_on_idle: true

  # Resource thresholds (warnings)
  cpu_threshold_percent: 95
  memory_threshold_percent: 90

  # Low-resource mode
  enable_low_resource_mode: false

  # GPU management
  gpu_memory_fraction: 0.8  # Use 80% of GPU memory max

# =============================================================================
# CLOUDEVENTS MULTI-BACKEND EVENT PUBLISHING (NEW)
# =============================================================================

events:
  # Global enable/disable for all event publishing
  enabled: true

  # Event filtering - specify which events to publish (optional)
  # If empty or null, all events are published
  # Available events: job.started, job.progress, job.paused, job.resumed, job.completed, job.failed, job.cancelled
  publish_events: null  # null = publish all events

  # Backend configurations
  backends:
    # PRIMARY: Redis Streams (low-latency, consumer group ready)
    - type: redis_streams
      enabled: true
      config:
        url: null  # Uses REDIS_CACHE_URL from environment
        stream_name: "stage1:cleaning:events"
        max_len: 10000  # Trim to last 10K events
        ttl_seconds: 86400  # 24 hours
        fail_silently: true

    # SECONDARY: Webhooks (HTTP callbacks to downstream stages)
    - type: webhook
      enabled: false  # Enable and configure URLs in production
      config:
        urls: []  # Add webhook URLs here, e.g., ["http://stage2-orchestrator:8000/webhooks/cleaning-completed"]
        headers: {}  # Optional headers, e.g., {"X-API-Key": "secret"}
        timeout_seconds: 30
        retry_attempts: 3
        fail_silently: true

    # OPTIONAL: Kafka (high-throughput for event-driven architectures)
    - type: kafka
      enabled: false
      config:
        bootstrap_servers: ["kafka:9092"]
        topic: "stage1.cleaning.events"
        compression_type: "gzip"
        fail_silently: true

    # OPTIONAL: NATS (cloud-native messaging)
    - type: nats
      enabled: false
      config:
        servers: ["nats://nats:4222"]
        subject: "stage1.cleaning.events"
        use_jetstream: true
        fail_silently: true

    # OPTIONAL: RabbitMQ (flexible message routing)
    - type: rabbitmq
      enabled: false
      config:
        url: "amqp://guest:guest@rabbitmq:5672/"
        exchange: "stage1.cleaning"
        routing_key: "events"
        exchange_type: "topic"
        fail_silently: true

# =============================================================================
# METADATA REGISTRY INTEGRATION (NEW)
# =============================================================================

metadata_registry:
  # Enable/disable metadata registry integration
  enabled: true

  # Primary backend (postgresql)
  primary_backend: postgresql

  # Enable Redis cache for metadata queries
  enable_redis_cache: true

  # Connection pooling
  pool_min_size: 2
  pool_max_size: 10

  # Retry configuration
  max_retries: 3
  retry_delay_seconds: 2

# config/settings.yaml
